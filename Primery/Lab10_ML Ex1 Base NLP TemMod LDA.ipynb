{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rFIgOM6i3Al"
   },
   "source": [
    "# Лабораторная работа №10. Основы обработки естественного языка (NLP). Задача тематического моделирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDtmLvThDHOv"
   },
   "source": [
    "**Обработка естественного языка (NLP)** – это технология ML, которая дает компьютерам возможность интерпретировать, манипулировать и понимать  естественный(человеческий) язык (ЕЯ). Сегодня организации имеют большие объемы голосовых и текстовых данных из различных каналов связи, таких как электронные письма, текстовые сообщения, новостные ленты социальных сетей, видео, аудио и многое другое. Они используют программное обеспечение NLP для автоматической обработки этих данных, анализа намерений или настроений в сообщении и реагирования на человеческое общение в режиме реального времени.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZULRBeHwDPK6"
   },
   "source": [
    "NLP сочетает в себе компьютерную лингвистику, ML и модели DL для обработки ЕЯ.\n",
    "\n",
    "**Компьютерная лингвистика**\n",
    "<br>Компьютерная лингвистика – это наука о понимании и построении моделей человеческого языка с помощью компьютеров и программных инструментов. Исследователи используют методы компьютерной лингвистики, такие как синтаксический и семантический анализ, для создания платформ, помогающих машинам понимать разговорный человеческий язык. Такие инструменты, как переводчики языков, синтезаторы текста в речь и программное обеспечение для распознавания речи, основаны на компьютерной лингвистике.\n",
    "\n",
    "**Машинное обучение**\n",
    "<br>ML – это технология, которая обучает компьютер с помощью выборочных данных для повышения его эффективности. Человеческий язык имеет несколько особенностей, таких как сарказм, метафоры, вариации в структуре предложений, а также исключения из грамматики и употребления, на изучение которых у людей уходят годы. Программисты используют методы машинного обучения, чтобы научить приложения NLP распознавать и точно понимать эти функции с самого начала.\n",
    "\n",
    "**Глубокое обучение**\n",
    "<br>DL – это особая область ML, которая учит компьютеры учиться и мыслить как люди. Это включает нейросеть, состоящую из узлов обработки данных, напоминающих операции человеческого мозга. С помощью глубокого обучения компьютеры распознают, классифицируют и сопоставляют сложные закономерности во входных данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeR8-yebDVdY"
   },
   "source": [
    "## Этапы обработки\n",
    "В NLP используются *методы предварительной обработки*, такие как:\n",
    "1. токенизация;\n",
    "2. стемминг, лемматизация;\n",
    "3. удаление стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mcWYv6LJU9m"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!pip install -U spacy-lookups-data\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHhgJ-tUJfx4"
   },
   "source": [
    "## Токенизация (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErMWKmzhJV8j",
    "outputId": "218e043e-a24a-4b7e-b741-294118277715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0: I\n",
      "Token 1: love\n",
      "Token 2: coding\n",
      "Token 3: and\n",
      "Token 4: writing\n"
     ]
    }
   ],
   "source": [
    "## tokenizing a piecen of text\n",
    "doc = \"I love coding and writing\"\n",
    "for i, w in enumerate(doc.split(\" \")):\n",
    "    print(\"Token \" + str(i) + \": \" + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LICJKScCQgcw"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqtsSWbNQnXw",
    "outputId": "ce1ceede-2586-4d31-82f4-3f871bdebc73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I, m, student]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp('Im student')\n",
    "[token for token in doc2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcXF43GnTCIs"
   },
   "source": [
    "## Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUpmCrvrS7G-",
    "outputId": "7828f1a4-d630-4fc7-9755-70d68e130c00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_stopwords = nlp.Defaults.stop_words\n",
    "type(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1CTVXkx0LEu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LBArOjUPT2AV"
   },
   "outputs": [],
   "source": [
    "my_stopwords.add('oops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63WNYQGvUH5b",
    "outputId": "1e73ae6d-5b70-4c32-bec0-6782f04f641a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[like, coffee]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I like coffee oops')\n",
    "[token for token in doc if token.lower_ not in my_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2-Mv75YJhjI"
   },
   "source": [
    "## Лемматизация (Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxqRXmzcJmnm",
    "outputId": "f66a459a-a702-40e3-abc5-058ecc11f71d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I => I\n",
      "love => love\n",
      "coding => code\n",
      "and => and\n",
      "writing => write\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'code', 'and', 'write']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lookups import Lookups\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "## lemmatization\n",
    "# токенизация (получение объекта doc)\n",
    "doc = nlp(u'I love coding and writing')\n",
    "# лемматизация\n",
    "for word in doc:\n",
    "    print(word.text, \"=>\", word.lemma_)\n",
    "data_lematized = [word.lemma_ for word in doc]\n",
    "data_lematized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iGjAE1qJu-O"
   },
   "source": [
    "## Стемминг (Stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xMFI52qQkxz"
   },
   "source": [
    "# Word embedding -- векторное представление слов\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JxrYbiDRWXB"
   },
   "source": [
    "## Мешок слов (Bag of words, BW)\n",
    "\n",
    "Рассмотрим самый простой способ приведения текста к набору чисел. Для каждого слова посчитаем, как часто оно встречается в тексте. Результаты запишем в таблицу. Строки будут представлять тексты, столбцы -- слова. Если на пересечении строки с столбца стоит число 5, значит данное слово встретилось в данном тексте 5 раз. В большинстве ячеек будут нули. Поэтому хранить это всё удобнее в виде разреженных матриц (т.е. хранить только ненулевые значения).\n",
    "\n",
    "Таким образом, при построении \"мешка слов\" можно выделить следующие действия:\n",
    "\n",
    "1. Токенизация.\n",
    "\n",
    "2. Построение словаря: собираем все слова, которые встречались в текстах и пронумеровываем их (по алфавиту, например).\n",
    "\n",
    "3. Построение разреженной матрицы. В sklearn алгоритм приведения текста в bag-of-words реализован в виде класса CountVectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PeWNrMvmRVjG",
    "outputId": "918624fa-f864-4dd0-8fe8-8c7c2279440f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 48)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "texts = [\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
    "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
    "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше.\"]\n",
    "\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d111Us5hSxzL"
   },
   "source": [
    "Результат содержит 3 строки (для 3 текстов) и 48 столбцов (для 48 разных слов). Посмотрим словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1io42iauSwTb",
    "outputId": "54bc67f1-36e5-4c24-cc2e-aa0bf57eb2a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'великолепный': 5,\n",
       " 'сериал': 36,\n",
       " 'который': 12,\n",
       " 'поможет': 27,\n",
       " 'успокоить': 46,\n",
       " 'нервы': 20,\n",
       " 'при': 29,\n",
       " 'любых': 15,\n",
       " 'стрессах': 43,\n",
       " 'просто': 30,\n",
       " 'скрасит': 39,\n",
       " 'серые': 38,\n",
       " 'будни': 2,\n",
       " 'пожалуй': 25,\n",
       " 'если': 10,\n",
       " 'бы': 4,\n",
       " 'посмотрел': 28,\n",
       " 'только': 45,\n",
       " 'первые': 24,\n",
       " 'пару': 23,\n",
       " 'сезонов': 35,\n",
       " 'этого': 47,\n",
       " 'сериала': 37,\n",
       " 'легкой': 14,\n",
       " 'руки': 33,\n",
       " 'написал': 18,\n",
       " 'ему': 9,\n",
       " 'положительную': 26,\n",
       " 'рецензию': 32,\n",
       " 'общем': 22,\n",
       " 'создатели': 41,\n",
       " 'не': 19,\n",
       " 'вернут': 6,\n",
       " 'всё': 8,\n",
       " 'на': 17,\n",
       " 'круги': 13,\n",
       " 'своя': 34,\n",
       " 'то': 44,\n",
       " 'рейтинги': 31,\n",
       " 'следующих': 40,\n",
       " 'будут': 3,\n",
       " 'становится': 42,\n",
       " 'все': 7,\n",
       " 'ниже': 21,\n",
       " 'зрительская': 11,\n",
       " 'аудитория': 0,\n",
       " 'будет': 1,\n",
       " 'меньше': 16}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQVMFmKSTBfD",
    "outputId": "7e0d6e86-5658-4df2-9d14-c728289f6586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 1],\n",
       "        [1, 1, 0, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 1, 0, 0, 2, 1, 0, 1, 0,\n",
       "         2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mECQhd7uTFqs"
   },
   "source": [
    "Как видим, ни стемминга, ни лемматизации по умолчанию не производится.\n",
    "Поэтому для уменьшения размерности данной матрицы требуется производить лемматизацию/стемминг и удаления стоп-слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEP3drzOTQgu"
   },
   "source": [
    "**Параметр min_df**\n",
    "\n",
    "Помимо выше озвученных есть и другие способы отсечения лишнего. Например, можно откидывать слова, которые встречаются слишком редко, с помощью параметра min_df. Установив min_df=2 мы откинем, все слова, которые встречаются менее, чем в 2 документах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUrVKE9eVvvK",
    "outputId": "d1c33c3f-10a0-4de2-8103-3e8ad2ac4f75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'если': 0, 'сезонов': 1, 'этого': 3, 'сериала': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=2)\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfA4vT56V_F5"
   },
   "source": [
    "**Биграммы, триграммы, n-граммы**\n",
    "\n",
    "По умолчанию bag-of-words (как следует из названия) представляет собой просто мешок слов. То есть для него предложения \"It's not good, it's bad!\" и \"It's not bad, it's good!\" абсолютно эквивалентны. Понятно, что при этом теряется много информации. Можно рассматривать не только отдельные слова, а последовательности длиной из 2 слов (биграммы), из 3 слов (триграммы) или в общем случае из n слов (n-граммы). На практике обычно задаётся диапазон от 1 до n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCpfC36oWBxB",
    "outputId": "59c7577a-548c-4018-e5f9-fc9a067439f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'если': 0, 'сезонов': 1, 'этого': 3, 'сериала': 2, 'этого сериала': 4}\n",
      "[[1 1 0 0 0]]\n",
      "[[0 2 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "print(count_vectorizer.vocabulary_)\n",
    "\n",
    "print(count_vectorizer.transform(['Если несколько сезонов']).todense())\n",
    "print(count_vectorizer.transform(['Этого сериала этого сезонов сезонов']).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riLA3FPAWRzP"
   },
   "source": [
    "**Ограничение количества признаков**\n",
    "\n",
    "Понятно, что с ростом n количество выделенных n-грамм быстро растёт. Для ограничения количества признаков можно использовать параметр max_features. В этом случае будет создано не более max_features признаков (будут выбраны самые часто встречающиеся слова и последовательности слов). Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64h04sKlWUH1",
    "outputId": "10d7b5b2-3bb8-41f1-d831-a04fb606bc3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'поможет': 7,\n",
       " 'при': 11,\n",
       " 'просто': 13,\n",
       " 'поможет успокоить': 8,\n",
       " 'при любых': 12,\n",
       " 'просто скрасит': 14,\n",
       " 'если': 2,\n",
       " 'бы': 0,\n",
       " 'посмотрел': 9,\n",
       " 'сезонов': 21,\n",
       " 'этого': 23,\n",
       " 'сериала': 22,\n",
       " 'руки': 17,\n",
       " 'положительную': 5,\n",
       " 'рецензию': 16,\n",
       " 'посмотрел только': 10,\n",
       " 'этого сериала': 24,\n",
       " 'руки написал': 18,\n",
       " 'положительную рецензию': 6,\n",
       " 'своя': 19,\n",
       " 'все': 1,\n",
       " 'ниже': 4,\n",
       " 'меньше': 3,\n",
       " 'своя то': 20,\n",
       " 'рейтинги следующих': 15}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2), max_features=25)\n",
    "bow = count_vectorizer.fit_transform(texts)\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoGGIlJ1Wf5Z"
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "У подхода bag-of-words есть существенный недостаток. Если слово встречается 5 раз в конкретном документе, но и в других документах тоже встречается часто, то его наличие в документе не особо-то о чём-то говорит. Если же слово 5 раз встречается в конкретном документе, но в других документах встречается редко, то его наличие (да ещё и многократное) позволяет хорошо отличать этот документ от других. Однако с точки зрения bag-of-words различий не будет: в обеих ячейках будет просто число 5.\n",
    "\n",
    "Отчасти это решается исключением стоп-слов (и слишком часто встречающихся слов), но лишь отчасти. Другой идеей является отмасштабировать получившуюся таблицу с учётом \"редкости\" слова в наборе документов (т.е. с учётом информативности слова).\n",
    "\n",
    "$tfidf=tf∗idf$\n",
    "\n",
    "$idf=\\log\\frac{(N+1)}{(Nw+1)}+1$\n",
    "\n",
    "Здесь tf это частота слова в тексте (то же самое, что в bag of words), N - общее число документов, Nw - число документов, содержащих данное слово.\n",
    "\n",
    "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки.\n",
    "\n",
    "В sklearn есть класс для поддержки TF-IDF: TfidfVectorizer, рассмотрим его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioHGsp6AYkM3",
    "outputId": "ba67ddc6-ca97-44c4-d03b-bc0fda5812ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'великолепный': 5,\n",
       " 'сериал': 36,\n",
       " 'который': 12,\n",
       " 'поможет': 27,\n",
       " 'успокоить': 46,\n",
       " 'нервы': 20,\n",
       " 'при': 29,\n",
       " 'любых': 15,\n",
       " 'стрессах': 43,\n",
       " 'просто': 30,\n",
       " 'скрасит': 39,\n",
       " 'серые': 38,\n",
       " 'будни': 2,\n",
       " 'пожалуй': 25,\n",
       " 'если': 10,\n",
       " 'бы': 4,\n",
       " 'посмотрел': 28,\n",
       " 'только': 45,\n",
       " 'первые': 24,\n",
       " 'пару': 23,\n",
       " 'сезонов': 35,\n",
       " 'этого': 47,\n",
       " 'сериала': 37,\n",
       " 'легкой': 14,\n",
       " 'руки': 33,\n",
       " 'написал': 18,\n",
       " 'ему': 9,\n",
       " 'положительную': 26,\n",
       " 'рецензию': 32,\n",
       " 'общем': 22,\n",
       " 'создатели': 41,\n",
       " 'не': 19,\n",
       " 'вернут': 6,\n",
       " 'всё': 8,\n",
       " 'на': 17,\n",
       " 'круги': 13,\n",
       " 'своя': 34,\n",
       " 'то': 44,\n",
       " 'рейтинги': 31,\n",
       " 'следующих': 40,\n",
       " 'будут': 3,\n",
       " 'становится': 42,\n",
       " 'все': 7,\n",
       " 'ниже': 21,\n",
       " 'зрительская': 11,\n",
       " 'аудитория': 0,\n",
       " 'будет': 1,\n",
       " 'меньше': 16}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmKX6oGcYqSm"
   },
   "source": [
    "Словарь содержит те же 48 значений, которые были бы и для CountVectorizer. Но значения в таблице другие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cpgf0X77YmNn",
    "outputId": "3247a990-aa1e-4e9a-bd94-0cee353fef21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.2773501 , 0.        , 0.        ,\n",
       "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.2773501 , 0.        , 0.        ,\n",
       "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.2773501 , 0.        , 0.2773501 ,\n",
       "         0.2773501 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2773501 , 0.        , 0.2773501 , 0.2773501 ,\n",
       "         0.        , 0.        , 0.        , 0.2773501 , 0.        ,\n",
       "         0.        , 0.2773501 , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.48065817,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.24032909,\n",
       "         0.18277647, 0.        , 0.        , 0.        , 0.24032909,\n",
       "         0.        , 0.        , 0.        , 0.24032909, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.24032909, 0.24032909,\n",
       "         0.24032909, 0.24032909, 0.        , 0.24032909, 0.        ,\n",
       "         0.        , 0.        , 0.24032909, 0.24032909, 0.        ,\n",
       "         0.18277647, 0.        , 0.18277647, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.24032909, 0.        , 0.18277647],\n",
       "        [0.18162735, 0.18162735, 0.        , 0.18162735, 0.        ,\n",
       "         0.        , 0.18162735, 0.36325471, 0.18162735, 0.        ,\n",
       "         0.13813228, 0.18162735, 0.        , 0.18162735, 0.        ,\n",
       "         0.        , 0.36325471, 0.18162735, 0.        , 0.18162735,\n",
       "         0.        , 0.36325471, 0.18162735, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.18162735, 0.        , 0.        , 0.18162735,\n",
       "         0.13813228, 0.        , 0.13813228, 0.        , 0.        ,\n",
       "         0.18162735, 0.18162735, 0.18162735, 0.        , 0.18162735,\n",
       "         0.        , 0.        , 0.13813228]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mODMPXvhZBbo"
   },
   "source": [
    "Ненулевые значения находятся на тех же местах, но отмасштабированы в зависимости от частоты слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1k2kwuOZFj3"
   },
   "source": [
    "**Параметр sublinear_tf**\n",
    "\n",
    "Большая часть параметров у CountVectorizer и TfidfVectorizer одинакова. Но у TfidfVectorizer есть один важный дополнительный параметр.\n",
    "\n",
    "Как видно из формулы tfidf = tf * idf, если слово будет встречаться не один, а два раза, то tfidf вырастет в два раза. Если слово будет встречаться не один, а 10 раз, то tfidf вырастет почти в 10 раз. В качестве примера добавим в третью строку ещё пару слов меньше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0F-HviBYvoA",
    "outputId": "b67e7014-be58-4260-a111-efdf1e508a68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.15373049, 0.15373049, 0.        , 0.15373049, 0.        ,\n",
       "         0.        , 0.15373049, 0.30746099, 0.15373049, 0.        ,\n",
       "         0.116916  , 0.15373049, 0.        , 0.15373049, 0.        ,\n",
       "         0.        , 0.61492198, 0.15373049, 0.        , 0.15373049,\n",
       "         0.        , 0.30746099, 0.15373049, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.15373049, 0.        , 0.        , 0.15373049,\n",
       "         0.116916  , 0.        , 0.116916  , 0.        , 0.        ,\n",
       "         0.15373049, 0.15373049, 0.15373049, 0.        , 0.15373049,\n",
       "         0.        , 0.        , 0.116916  ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"Великолепный сериал, который поможет успокоить нервы при любых стрессах и просто скрасит серые будни\",\n",
    "         \"Пожалуй, если бы я посмотрел только первые пару сезонов этого сериала, я бы с легкой руки написал ему положительную рецензию\",\n",
    "         \"В общем, если создатели этого сериала не вернут всё на круги своя, то рейтинги следующих сезонов будут становится все ниже и ниже, а зрительская аудитория будет все меньше и меньше и меньше и меньше.\"]\n",
    "TfidfVectorizer().fit_transform(texts).todense()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfDTxTa2ZSWH"
   },
   "source": [
    "Значение tfidf слова \"меньше\" выросло с 0.36325471 до 0.61492198, а остальные упали .\n",
    "\n",
    "Вопрос - хотим ли мы таких сильных изменений. Если не хотим, то можно использовать параметр sublinear_tf=True. При его использовании вместо tf будет браться 1 + log(tf). То есть по-прежнему с ростом tf будет расти и tfidf, но уже не так радикально (и соответственно остальные значения будут уменьшаться не так быстро). Для некоторых задач это может дать прирост в качестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBgHVX_pZKsY",
    "outputId": "82e13cd2-6cb0-4b61-ea40-fc3ce6d7cd6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.18336592, 0.18336592, 0.        , 0.18336592, 0.        ,\n",
       "         0.        , 0.18336592, 0.31046549, 0.18336592, 0.        ,\n",
       "         0.13945451, 0.18336592, 0.        , 0.18336592, 0.        ,\n",
       "         0.        , 0.43756505, 0.18336592, 0.        , 0.18336592,\n",
       "         0.        , 0.31046549, 0.18336592, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.18336592, 0.        , 0.        , 0.18336592,\n",
       "         0.13945451, 0.        , 0.13945451, 0.        , 0.        ,\n",
       "         0.18336592, 0.18336592, 0.18336592, 0.        , 0.18336592,\n",
       "         0.        , 0.        , 0.13945451]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVectorizer(sublinear_tf=True).fit_transform(texts).todense()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLI-fF73bUdn",
    "outputId": "781cc7fc-c8e5-4a05-e17f-370e132bd287"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGAL_G_zDupn"
   },
   "source": [
    "# LDA\n",
    "\n",
    "LDA принадлежит семейству порождающий вероятностных моделей, в которых темы представлены вероятностями появления каждого слова из заданного набора. Документы в свою очередь могут быть представлены как сочетания тем. Уникальная особенность моделей LDA состоит в том что темы не обязательно должны быть различными и слова могут встречаться в нескольких темах; это придает некоторую нечеткость определяемым темам, что может пригодиться для совладения с гибкостью языка.\n",
    "\n",
    "Для проведения тематического моделирования с помощью LDA можно использовать [sklearn.decomposition.LatentDirichletAllocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "x4uSDKYwEwx3"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yIH6LoX6E40P"
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_o-owByVH5zB"
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0BJQUwMH6Z5",
    "outputId": "d4590da9-5de0-46de-dc71-1b89a9e73593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4jL4yebIILz",
    "outputId": "c243428a-b37d-4a16-c9d7-09ab062fc9ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqn03YEof36y",
    "outputId": "297f1e48-ee0c-45a6-9536-a08e4d4866e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xQftgYw8IAvw"
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdjMq9E9IS6K",
    "outputId": "778771ab-5f18-4a13-8807-334f85a07254"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '10', '100', '11', '12', '128', '13', '130', '14',\n",
       "       '15', '16', '17', '18', '19', '1992', '1993', '20', '200', '21',\n",
       "       '22', '23', '24', '25', '250', '26', '27', '28', '29', '2nd', '30',\n",
       "       '300', '31', '32', '33', '34', '35', '36', '37', '38', '3d', '40',\n",
       "       '42', '43', '44', '45', '48', '49', '50', '500', '51', '55', '60',\n",
       "       '66', '70', '72', '75', '80', '800', '86', '90', '92', '93', '__',\n",
       "       'able', 'ac', 'accept', 'access', 'according', 'act', 'action',\n",
       "       'actually', 'add', 'added', 'addition', 'address',\n",
       "       'administration', 'advance', 'age', 'ago', 'agree', 'aids', 'air',\n",
       "       'al', 'allow', 'allowed', 'alt', 'america', 'american', 'amiga',\n",
       "       'analysis', 'anonymous', 'answer', 'answers', 'anti', 'anybody',\n",
       "       'apartment', 'appears', 'apple', 'application', 'applications',\n",
       "       'apply', 'appreciated', 'approach', 'appropriate', 'apr', 'april',\n",
       "       'archive', 'area', 'areas', 'aren', 'argument', 'armenia',\n",
       "       'armenian', 'armenians', 'army', 'article', 'ask', 'asked',\n",
       "       'asking', 'assume', 'atheism', 'attack', 'attacks', 'attempt',\n",
       "       'au', 'author', 'authority', 'available', 'average', 'away',\n",
       "       'azerbaijan', 'bad', 'based', 'basic', 'basically', 'begin',\n",
       "       'belief', 'believe', 'best', 'better', 'bible', 'big', 'bike',\n",
       "       'billion', 'bios', 'bit', 'black', 'block', 'blood', 'blue',\n",
       "       'board', 'bob', 'body', 'book', 'books', 'bought', 'box', 'brake',\n",
       "       'break', 'bring', 'btw', 'build', 'building', 'built', 'bus',\n",
       "       'business', 'buy', 'buying', 'ca', 'cable', 'called', 'calls',\n",
       "       'came', 'canada', 'car', 'card', 'cards', 'care', 'carry', 'cars',\n",
       "       'case', 'cases', 'cause', 'cc', 'cd', 'center', 'certain',\n",
       "       'certainly', 'chance', 'change', 'changed', 'changes', 'cheap',\n",
       "       'check', 'children', 'chip', 'choice', 'christ', 'christian',\n",
       "       'christians', 'church', 'citizens', 'city', 'claim', 'clear',\n",
       "       'clearly', 'clinton', 'clipper', 'clock', 'close', 'code', 'cold',\n",
       "       'color', 'com', 'come', 'comes', 'coming', 'command', 'comments',\n",
       "       'commercial', 'common', 'communications', 'community', 'comp',\n",
       "       'company', 'complete', 'completely', 'computer', 'condition',\n",
       "       'conference', 'congress', 'connector', 'consider', 'considered',\n",
       "       'contact', 'containing', 'contains', 'continue', 'control',\n",
       "       'controller', 'copies', 'copy', 'correct', 'cost', 'costs',\n",
       "       'couldn', 'countries', 'country', 'couple', 'course', 'court',\n",
       "       'cover', 'create', 'created', 'crime', 'crowd', 'cs', 'cubs',\n",
       "       'current', 'currently', 'cut', 'dangerous', 'data', 'database',\n",
       "       'date', 'dave', 'david', 'day', 'days', 'dc', 'dead', 'deal',\n",
       "       'death', 'dec', 'decided', 'defense', 'deleted', 'department',\n",
       "       'design', 'designed', 'details', 'developed', 'development',\n",
       "       'device', 'devices', 'did', 'didn', 'die', 'died', 'difference',\n",
       "       'different', 'difficult', 'digital', 'directly', 'directory',\n",
       "       'discussion', 'disease', 'disk', 'display', 'division', 'does',\n",
       "       'doesn', 'dog', 'doing', 'dollars', 'don', 'door', 'dos', 'dot',\n",
       "       'doubt', 'dr', 'drive', 'driver', 'drivers', 'drives', 'driving',\n",
       "       'drug', 'earlier', 'early', 'earth', 'easily', 'easy', 'edu',\n",
       "       'effect', 'effective', 'effort', 'email', 'encrypted',\n",
       "       'encryption', 'end', 'energy', 'enforcement', 'engine', 'entire',\n",
       "       'equipment', 'eric', 'error', 'especially', 'events', 'evidence',\n",
       "       'exactly', 'example', 'excellent', 'exist', 'expect', 'experience',\n",
       "       'explain', 'extra', 'face', 'fact', 'fair', 'fairly', 'faith',\n",
       "       'family', 'faq', 'far', 'fast', 'faster', 'father', 'fax',\n",
       "       'feature', 'features', 'federal', 'feel', 'field', 'figure',\n",
       "       'file', 'files', 'final', 'finally', 'fine', 'firearm', 'fit',\n",
       "       'floppy', 'flyers', 'folks', 'follow', 'following', 'food',\n",
       "       'force', 'forget', 'form', 'format', 'formats', 'free', 'freedom',\n",
       "       'friend', 'ftp', 'function', 'future', 'game', 'games', 'gas',\n",
       "       'gave', 'gay', 'general', 'generally', 'gets', 'getting', 'given',\n",
       "       'gives', 'giving', 'gm', 'goal', 'god', 'goes', 'going', 'gone',\n",
       "       'good', 'got', 'gov', 'government', 'graphics', 'great', 'greek',\n",
       "       'ground', 'group', 'groups', 'guess', 'gun', 'guns', 'guy', 'half',\n",
       "       'hand', 'happen', 'happened', 'happens', 'happy', 'hard',\n",
       "       'hardware', 'haven', 'having', 'head', 'heads', 'health', 'hear',\n",
       "       'heard', 'heart', 'heaven', 'held', 'hell', 'help', 'hi', 'high',\n",
       "       'higher', 'history', 'hit', 'hiv', 'hockey', 'hold', 'home',\n",
       "       'hope', 'hospital', 'hot', 'hours', 'house', 'hp', 'human', 'ibm',\n",
       "       'id', 'idea', 'ii', 'image', 'images', 'imagine', 'important',\n",
       "       'include', 'included', 'includes', 'including', 'increase',\n",
       "       'individual', 'info', 'information', 'input', 'inside', 'instead',\n",
       "       'insurance', 'interested', 'interesting', 'interface', 'internal',\n",
       "       'international', 'internet', 'involved', 'isn', 'israel',\n",
       "       'israeli', 'issue', 'james', 'jesus', 'jewish', 'jews', 'job',\n",
       "       'jobs', 'john', 'just', 'kept', 'key', 'keys', 'kill', 'killed',\n",
       "       'killing', 'kind', 'knew', 'know', 'knowledge', 'known', 'knows',\n",
       "       'lack', 'land', 'language', 'large', 'late', 'later', 'latest',\n",
       "       'launch', 'law', 'laws', 'leafs', 'learn', 'leave', 'led', 'left',\n",
       "       'legal', 'let', 'letter', 'level', 'library', 'license', 'life',\n",
       "       'light', 'like', 'likely', 'limited', 'line', 'lines', 'list',\n",
       "       'listen', 'little', 'live', 'lives', 'living', 'll', 'local',\n",
       "       'long', 'longer', 'look', 'looking', 'looks', 'lord', 'lost',\n",
       "       'lot', 'lots', 'love', 'low', 'luck', 'lunar', 'mac', 'machine',\n",
       "       'machines', 'magi', 'mail', 'main', 'major', 'make', 'makes',\n",
       "       'making', 'mamma', 'man', 'manager', 'manual', 'mark', 'market',\n",
       "       'marriage', 'mars', 'mary', 'mass', 'master', 'math', 'matter',\n",
       "       'maybe', 'mb', 'mean', 'meaning', 'means', 'media', 'medical',\n",
       "       'member', 'members', 'memory', 'men', 'mention', 'mentioned',\n",
       "       'message', 'middle', 'mike', 'mil', 'miles', 'military', 'million',\n",
       "       'mind', 'mission', 'mit', 'mode', 'model', 'models', 'modern',\n",
       "       'money', 'monitor', 'months', 'moon', 'moral', 'mother', 'motif',\n",
       "       'mr', 'ms', 'nasa', 'national', 'nature', 'navy', 'near',\n",
       "       'necessary', 'need', 'needed', 'needs', 'net', 'network', 'new',\n",
       "       'news', 'newsgroup', 'nhl', 'nice', 'night', 'non', 'normal',\n",
       "       'north', 'note', 'nsa', 'number', 'numbers', 'objects', 'obvious',\n",
       "       'offer', 'office', 'oh', 'oil', 'ok', 'old', 'older', 'ones',\n",
       "       'open', 'opinion', 'opinions', 'orbit', 'order', 'org',\n",
       "       'organization', 'original', 'os', 'output', 'outside', 'package',\n",
       "       'page', 'paper', 'papers', 'parents', 'particular', 'parts',\n",
       "       'party', 'pass', 'past', 'paul', 'pay', 'pc', 'people',\n",
       "       'performance', 'period', 'person', 'personal', 'peter', 'phone',\n",
       "       'pick', 'piece', 'pin', 'pittsburgh', 'place', 'places', 'plan',\n",
       "       'play', 'played', 'player', 'players', 'playing', 'plus', 'point',\n",
       "       'points', 'police', 'policy', 'political', 'population', 'port',\n",
       "       'position', 'possible', 'post', 'posted', 'posting', 'power', 'pp',\n",
       "       'present', 'president', 'press', 'pretty', 'price', 'printer',\n",
       "       'private', 'pro', 'probably', 'probe', 'probes', 'problem',\n",
       "       'problems', 'process', 'product', 'program', 'programs', 'project',\n",
       "       'prove', 'provide', 'pub', 'public', 'purpose', 'putting',\n",
       "       'quality', 'question', 'questions', 'quite', 'radio', 'ram',\n",
       "       'range', 'rate', 'rates', 'ray', 'read', 'reading', 'real',\n",
       "       'really', 'reason', 'reasonable', 'received', 'recent', 'recently',\n",
       "       'recommend', 'record', 'red', 'reference', 'regular', 'related',\n",
       "       'release', 'religion', 'religious', 'remember', 'reply', 'report',\n",
       "       'request', 'require', 'required', 'research', 'response', 'rest',\n",
       "       'result', 'results', 'return', 'right', 'rights', 'road', 'robert',\n",
       "       'role', 'rom', 'room', 'rules', 'run', 'running', 'runs', 'safety',\n",
       "       'said', 'sale', 'san', 'save', 'saw', 'say', 'saying', 'says',\n",
       "       'school', 'sci', 'science', 'scientific', 'screen', 'scsi',\n",
       "       'search', 'season', 'second', 'section', 'secure', 'security',\n",
       "       'seen', 'self', 'sell', 'send', 'sense', 'sent', 'serial',\n",
       "       'series', 'seriously', 'server', 'service', 'set', 'sex', 'sgi',\n",
       "       'shall', 'short', 'shot', 'shots', 'shows', 'shuttle', 'signal',\n",
       "       'similar', 'simple', 'simply', 'sin', 'single', 'site',\n",
       "       'situation', 'size', 'slow', 'small', 'society', 'software',\n",
       "       'solar', 'sold', 'soldiers', 'soon', 'sorry', 'sort', 'sound',\n",
       "       'sounds', 'source', 'sources', 'soviet', 'space', 'spacecraft',\n",
       "       'special', 'specific', 'specifically', 'speed', 'st', 'standard',\n",
       "       'start', 'started', 'starting', 'state', 'statement', 'states',\n",
       "       'station', 'stay', 'stop', 'story', 'street', 'strong', 'study',\n",
       "       'stuff', 'stupid', 'subject', 'suggest', 'sun', 'support',\n",
       "       'supported', 'supports', 'suppose', 'supposed', 'sure', 'surface',\n",
       "       'switch', 'systems', 'taken', 'takes', 'taking', 'talk', 'talking',\n",
       "       'tape', 'team', 'teams', 'technical', 'technology', 'tell', 'term',\n",
       "       'test', 'text', 'thank', 'thanks', 'theory', 'thing', 'things',\n",
       "       'think', 'thinking', 'thought', 'time', 'times', 'tires', 'today',\n",
       "       'told', 'took', 'toronto', 'total', 'town', 'traffic', 'transfer',\n",
       "       'tried', 'trouble', 'true', 'trust', 'truth', 'try', 'trying',\n",
       "       'turkish', 'turn', 'turned', 'tv', 'type', 'types', 'uk',\n",
       "       'understand', 'unfortunately', 'united', 'university', 'unix',\n",
       "       'unless', 'use', 'used', 'useful', 'user', 'users', 'uses',\n",
       "       'using', 'usually', 'value', 'van', 'various', 've', 'version',\n",
       "       'vga', 'video', 'view', 'volume', 'vs', 'want', 'wanted', 'wants',\n",
       "       'war', 'washington', 'wasn', 'water', 'way', 'weapon', 'weapons',\n",
       "       'week', 'weeks', 'went', 'western', 'white', 'wife', 'willing',\n",
       "       'win', 'window', 'windows', 'wish', 'woman', 'women', 'won',\n",
       "       'wonder', 'wondering', 'word', 'words', 'work', 'worked',\n",
       "       'working', 'works', 'world', 'worse', 'worth', 'wouldn', 'write',\n",
       "       'written', 'wrong', 'xfree86', 'year', 'years', 'yes', 'young'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fTjSFEehCCa",
    "outputId": "696a1662-f66f-4f85-f6c2-5ef03a713591"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# матрица темы-слова\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5utkTYOBh_cQ",
    "outputId": "83e93c5c-a9a0-479f-c461-c71f5ded9799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# матрица документы-темы\n",
    "lda.transform(tf).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtN1TO9ziLKI",
    "outputId": "1419c95e-0fb8-40b0-d1b3-d4eef32285ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "edu com mail send graphics ftp pub available contact version\n",
      "Topic #1:\n",
      "don like just think know ve good way people right\n",
      "Topic #2:\n",
      "think christian book atheism new pittsburgh president like game just\n",
      "Topic #3:\n",
      "drive windows disk thanks use card drives hard using software\n",
      "Topic #4:\n",
      "hiv health aids april disease research care medical information 1993\n",
      "Topic #5:\n",
      "god people does jesus law say just life don israel\n",
      "Topic #6:\n",
      "10 55 11 15 game 12 18 team 20 19\n",
      "Topic #7:\n",
      "car year new bike cars good engine just price oil\n",
      "Topic #8:\n",
      "people said didn went did know time just like took\n",
      "Topic #9:\n",
      "key space government public use law encryption section keys earth\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, tf_vectorizer.get_feature_names_out(), 10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
